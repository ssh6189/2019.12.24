분류 : 예측 대상의 속성(설명 변수)을 입력받아서 목표변수(종속변수)가 갖고 있는 카테고리(범주형)값 중 하나의 값으로 예측하는 분석 방법

분류 분석 유형(알고리즘) - KNN, SVM, Decision Tree, Logistic Regression

KNN - 새로운 관측값이 주어지면, 기존의 데이터 중에서 가장 속성이 비슷한 k개의 이웃을 먼저 찾아서 가까운 이웃들이 갖고 있는 목표변수(종속변수)와같은 값으로 분류, 예측
최근접점을 몇개로 할것인지 지정하는 K는 홀수, 작은 값 설정 권장

KNN은 기존 데이터 사이의 거리를 재서 이웃을 뽑는다. =>거리 측정 방법에 따라 결과가 달라진다.

#설명변수들을 정규화 (sklearn.preprocessing 모듈의 standardScaler())
훈련데이터와 검증 데이터 분리 - sklearn.model selection, sklearn.preprocessing모듈의 train_train_split()
학습(훈련)으로부터 만들어진 모델의 예측 능력(정확도, 재현율, F1) 평가 - confusion_matrix()

예측 능력(정확도, 재현율, F1) 평가 지표 계산 - sklearn.metrics 모듈의 classification_report()

TP  FP
FN TN

정확도 : TP/(TP + FP)

재현율 : TP/(TP + FN)

F1 = 2*정확도*재현율/(정확도 + 재현율)



KNN의 하이퍼 파라미터 - K개, 거리측정 방법

K가 작을 경우, 오버피팅
K가 커질 경우, 언더피팅



SVM
데이터를 오직 공간상의 정보(선)만으로 이진 분류 분석 할 때 사용
데이터간의 마진을 최대가 되는 선을 그어 분류

3차원 데이터, 2차원의 면으로 데이터를 집단분류 - 커널 트릭, 커널 함수 매핑

SVM은 동일한 분류 값을 갖는 데이터끼리 같은 공간에 위치하도록 벡터 공간을 여러 조각으로 나눌 수 있는 분류 방법

SVM은 노이즈에 영향을 받지 않으며, overfiting될 가능성이 낮음

SVM의 하이퍼파라미터 - cost, gamma(r), kernal



Decision Tree
한번에 하나씩 설명변수를 사용해서 예측가능한 규칙들의 집합을 생성하는 알고리즘

데이터를 분석해서 데이터 사이에 존재하는 패턴을 존재하는 패턴을 예측 가능ㅎ나 규칙들의 조합으로 표현 -> 나무모양

rootnode -> 규칙조건으로 분기 -> leaf node 또는 terminal node라고 한다.(마지막)
terminal node들의 데이터 개수의 합 = root node의 데이터 개수
분류된 데이터 집합 수는 terminal node의 수가 된다.
범주형 데이터, 연속형 수치 데이터 모두 분류, 예측 가능한 분석 방법
sklearn.tree 모듈의 DecisionTreeClassifier()

DecisionTree의 하이퍼 파라미터 - 각 노드의 순도는 증가시키고, 불순도(불확실성)은 감소
이를 위해 Gini계수, Entropy, 분류오차값 사용

분류오차(가지, 분기 기준)



군집(cluster)
데이터(관측값)이 가지는 여러 속성을 분석해서 서로 비슷한 특성을 갖는 데이터끼리 그룹핑하는 알고리즘
클러스터 내의 데이터들은 유사한 속성을 가지며 다른 클러스터와의 속성은 완전히 구별되는 속성을 가진다.
정답이 없는 비지도 학습

군집분석에서 대표적으로 사용하는것
K-Means : 유사한 데이터들의 클러스터 중심점까지의 거리를 이용
sklearn.cluster 모듈 - KMeans(n_clusters =,)
분류된 클러스터 결과 값은 KMeans객체.labels_